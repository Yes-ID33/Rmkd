---
title: "AnalisisEstadisticoTaller2"
output: 
  html_document:
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    collapsed: TRUE
    smooth_scroll: TRUE
    theme: cerulean
    df_print: paged
    code_folding: show
    
date: "2024-10-24"

author: "Cristina Sierra"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(tidyverse)
library(plotly)
library(MASS)
library(psych)
library(factoextra)
library(GGally)
library(dplyr)
library(ggplot2)
library(cluster)


```

# Visualización de los datos

```{r}

dfEnergiaPaises <- read.csv("./datos_taller_02.csv", header = TRUE, sep=';')

dfEnergiaPaises

```

#### Para este trabajo se utilizaron como 5 variables de interes, variables relacionadas con el consumo de energias de todo tipo, tanto fosiles como de electricidad.


# **Primera parte** 
> (Los valores atipicos existen, por lo cual multiples filas de datos pueden tener una afeccion significativa a datos medios)

## Punto 1
```{r}


# Select the 5 variables of interest
selected_data <- dfEnergiaPaises[, c("Distillate_fuel_oil_consumption_TBPD","Jet_fuel_consumption_TBPD", "Motor_gasoline_consumption_TBPD","Petroleum_and_other_liquids_consumption_TBPD","Electricity_net_consumption_BKWH")]

# Calculate the mean vector
prom_vector <- colMeans(selected_data, na.rm = TRUE)

# Calculate the variance-covariance matrix
var_cov_matrix <- cov(selected_data, use = "complete.obs")

# Display the results
print("Vector de los promedios:")
print(prom_vector)

print("Variance-Covariance Matrix:")
print(var_cov_matrix)

```

## Punto 2

```{r}
#Análisis descriptivo univariado
summary(selected_data)

#Análisis descriptivo bivariado relaciones par-par

ggpairs(selected_data)

```

## Punto 3

```{r}
# Remove country column (assuming the first column contains country names)
# We'll only apply PCA to the numeric variables
df_numeric <- dfEnergiaPaises[, -1] # This removes the first column (country names)

# Standardizing the data for PCA
standardized_data <- scale(df_numeric)

# PCA with standardized data
pca_standardized <- prcomp(standardized_data, scale = TRUE)
fviz_pca_var(pca_standardized, axes = c(1, 2), repel=TRUE, labelsize=2)

# PCA without standardization
pca_non_standardized <- prcomp(df_numeric, scale = FALSE)
fviz_pca_var(pca_non_standardized, axes = c(1, 2), repel=TRUE, labelsize=2)


```

### Datos estandarizaddos

```{r}
summary(pca_standardized)
```

### Datos **NO** estandarizados

```{r}
summary(pca_non_standardized)

```



## Punto 4

```{r}
# Visualizing the cumulative variance explained by each principal component
fviz_eig(pca_standardized)

# Using Kaiser criterion to select components with eigenvalues > 1
eigenvalues <- pca_standardized$sdev^2
selected_components <- sum(eigenvalues > 1)
selected_components
```
> 4. conclusión

>Basado en el PCA normado (estandarizado), deberíamos seleccionar el número de componentes principales basándonos en los siguientes criterios:

> 1. **Criterio de Kaiser**: Debemos seleccionar los componentes cuyos eigenvalores sean mayores a 1. 
    - Los componentes con eigenvalores mayores a 1 explican más varianza que cualquier variable original y se consideran significativos.
    - Esto ayuda a reducir la dimensionalidad mientras se conservan componentes significativos. Después de calcular los eigenvalores, retenemos aquellos donde λ > 1.

> 2. **Umbral de Varianza Explicada**: Otro enfoque es retener suficientes componentes para explicar una proporción suficiente de la varianza (por ejemplo, 80% o 90% de la varianza acumulada).
    - Se puede visualizar la varianza explicada usando un scree plot y observar el punto donde agregar más componentes comienza a contribuir muy poca varianza adicional (punto de "codo").
    - El número de componentes seleccionados bajo este criterio es donde alcanzamos el porcentaje deseado de varianza acumulada.

> Decisión Final: Usando tanto el criterio de Kaiser como el umbral de varianza explicada, decidimos el número óptimo de componentes, equilibrando la simplicidad y la preservación de la varianza.

## Punto 5

```{r}
# Load necessary libraries
library(factoextra)

# Perform PCA on the standardized dataset (excluding the country column)
df_numeric <- dfEnergiaPaises[, -1] # Remove country column
pca_result <- prcomp(scale(df_numeric))

# Visualize the variables on the first factorial plane (PC1 vs PC2)
# Selecting only the top 30 most contributing variables to avoid clutter
fviz_pca_var(pca_result, 
             axes = c(1, 2),        # Focus on PC1 and PC2
             select.var = list(contrib = 12),  # Select the top 30 contributing variables
             repel = TRUE,           # Avoid label overlap
             labelsize = 2)          # Decrease label size to avoid saturation

```
> 5. Conclusión:

> Este análisis nos permite visualizar cómo las variables originales están representadas en el primer plano factorial, formado por las dos primeras componentes principales (PC1 y PC2). Al filtrar las 30 variables que más contribuyen y ajustar el tamaño de las etiquetas, podemos identificar mejor cuáles son las variables que tienen la mayor influencia en estas componentes.

>Las variables con flechas más largas o que se encuentran más alejadas del centro son las que más contribuyen a la variabilidad explicada por estas componentes principales. Estas componentes son importantes porque capturan la mayor parte de la varianza del conjunto de datos. A partir de este análisis, podemos concluir qué variables son clave para explicar las diferencias entre los países en términos de su consumo y producción energética.

## Punto 6
```{r}
# Visualización de los países (individuos) en el segundo plano factorial (PC1 vs PC2)
fviz_pca_ind(pca_result, 
             axes = c(1, 2),        # Nos centramos en PC1 y PC2
             repel = TRUE,           # Evita la superposición de etiquetas
             labelsize = 3)          # Ajuste del tamaño de las etiquetas

# Visualización de los países (individuos) en el primer plano factorial (PC3 vs PC4)
fviz_pca_ind(pca_result, 
             axes = c(3, 4),        # Nos centramos en PC1 y PC2
             repel = TRUE,           # Evita la superposición de etiquetas
             labelsize = 3)          # Ajuste del tamaño de las etiquetas

```

> 6 Conclusión:

> Se puede concluir que en los 2 distintos planos, hay 4 países(individuos) que destacan que son China(21), Rusia(79), Saudi Arabia(80), USA(104), ya que en ambos planos son muy atípicos, aunque en el segundo hay otros que también son atípicos pero en conjunto no llegan a los 4 anteriormente mencionados, siendo así que sin estos, 4 individuos, todos los países estarían en un rango promedio de no demasiado uso de las energías y/o combustibles


## Punto 7

```{r}
# Crear el nuevo dataframe excluyendo los países atípicos basados en sus índices (China, Rusia, Arabia Saudita, USA)
dfEnergiaPaisesBounded <- dfEnergiaPaises[-c(21, 79, 80, 104), ]

# Eliminar la columna de países para el ACP, ya que no es numérica
df_numeric_cleaned <- dfEnergiaPaisesBounded[, -1]  # Excluir la columna de nombres de países

# Estandarizar los datos (normado)
df_scaled_cleaned <- scale(df_numeric_cleaned)

# Realizar el ACP normado
pca_cleaned <- prcomp(df_scaled_cleaned)

# Visualizar la representación de las variables en el primer plano factorial (PC1 vs PC2)
fviz_pca_var(pca_cleaned, axes = c(1, 2), repel = TRUE, labelsize = 4)

# Visualizar la representación de los países (individuos) en el primer plano factorial (PC1 vs PC2)
fviz_pca_ind(pca_cleaned, axes = c(1, 2), repel = TRUE, labelsize = 4)

# Visualizar los países en el segundo plano factorial (PC2 vs PC3)
fviz_pca_ind(pca_cleaned, axes = c(2, 3), repel = TRUE, labelsize = 4)
```

> 7. Respuestas:

> al tener menos valores atípicos es más fácil visualizar grupos con distancias más pequeñas entre si
se perciben clusters de países con mayor claridad? 

> Si, como las dimensiones ya tienen un menor porcentaje de peso total en cuanto a los datos, es más fácil ver grupos que coincidan entre D1 y D2 y sea más fácil ubicar un cluster de prueba uno mismo y agruparlos

# **Segunda parte** 
> (Eliminando Rusia, China, Arabia Saudita y USA debido a su afectacion a las correlaciones de los datos)

## Punto 8

```{r}
# Visualización del primer plano factorial (PC1 vs PC2)
fviz_pca_ind(pca_cleaned, axes = c(1, 2), repel = TRUE, labelsize = 4)

# Observamos el gráfico y contamos los posibles clusters que parecen formarse.

```

> 8. Respuestas:

> en donde hay más datos se pueden sacar 3 o 4 clusters, algunos atípicos podrían convertir este número en 5 o 6, incluso 7 si nos ponemos muy estrictos

## Punto 9

```{r}
# Definir un rango de clusters para analizar
max_clusters <- 10  # Cambié el límite a 10, ya que tienes una iteración hasta 11
wss <- numeric(max_clusters)  # Almacenar la suma de las distancias dentro de los clusters

# Calcular la suma de las distancias dentro de los clusters para cada número de clusters (k)
for (k in 1:max_clusters) {
  km_result <- kmeans(df_scaled_cleaned, centers = k, nstart = 33)
  wss[k] <- km_result$tot.withinss
}

# Graficar el método del codo
plot(1:max_clusters, wss[1:max_clusters], type = "b", pch = 19, frame = FALSE,
     xlab = "Número de Clusters (k)", 
     ylab = "Suma de las Distancias Dentro de los Clusters (WSS)")


```

> 9. conclusiones

> entre 4 y 6 clusters es la respuesta correcta


## Punto 10

```{r}

# K-means Clustering
set.seed(123)  # Para reproducibilidad
k <- 3  # Elegimos 4 clusters como ejemplo, ajústalo según el análisis anterior
kmeans_result <- kmeans(df_scaled_cleaned, centers = k, nstart = 25)

# Agregar los resultados de k-means al dataframe original
dfEnergiaPaisesBounded$KMeans_Group <- as.factor(kmeans_result$cluster)


# Aglomeración Jerárquica

# Calculando la distancia y realizando el clustering jerárquico
dist_matrix <- dist(df_scaled_cleaned)  # Matriz de distancias
hclust_result <- hclust(dist_matrix, method = "average")  # Aglomeración jerárquica
clusters_hclust <- cutree(hclust_result, k)  # Cortar en 3 clusters

# Agregar los resultados de aglomeración jerárquica al dataframe
dfEnergiaPaisesBounded$HCLUST_Group <- as.factor(clusters_hclust)

# Comparar los grupos obtenidos
table(dfEnergiaPaisesBounded$KMeans_Group, dfEnergiaPaisesBounded$HCLUST_Group)

```

> 10. conclusiones

> Se evidencia que en el cluster 2 es donde se agrupan la mayor cantidad de datos, es por eso que los clusters restantes cuentan con un valor menos significativo de datos

## Punto 11

```{r}
selected_data_scaled <- scale(selected_data)

# 3. Perform k-means clustering (you can adjust the value of k based on your analysis)
set.seed(123)  # Setting seed for reproducibility
kmeans_result <- kmeans(selected_data_scaled, centers = 3, nstart = 25)  # Using k = 3 as an example

# 4. Add the cluster labels to the original data
dfEnergiaPaises$cluster <- as.factor(kmeans_result$cluster)

# 5. Create boxplots for each variable, differentiated by cluster
# Loop through each variable to create a boxplot for each, grouped by clusters
for (variable in colnames(selected_data)) {
  p <- ggplot(dfEnergiaPaises, aes(x = cluster, y = .data[[variable]], fill = cluster)) +
    geom_boxplot() +
    labs(title = paste("Boxplot of", variable, "by K-means clusters"), 
         x = "Cluster", y = variable) +
    theme_minimal() +
    theme(legend.position = "none") +
    scale_fill_brewer(palette = "Set3")
  
  # Explicitly print the plot
  print(p)
}
```

> 11. conclusiones

> Es la comprobación gráfica de la distribución de los clusters en un boxplot




